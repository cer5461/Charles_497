---
title: "Homework 2 Submission"
author: "Charles Ryan"
date: "3/9/2021"
output: html_document
---


#1
##take a glimpse of data_corpus_inaugural
```
summary(data_corpus_inaugural)
```
## subset inaugurals corpus to only include speeches by Reagan 
```
reaganinaugurals <- corpus_subset(data_corpus_inaugural, President == "Reagan")
summary(reaganinaugurals)
```
#a. calculate the TTR of each of these speeches based on token/type data from summary(reaganinaugurals) and report findings 
## TTR 1981 
```
(902/2780)*100
```
### 1.a: the type-token ratio for Reagan's 1981 inaugural address is 32.45%
## TTR 1985
```
(925/2909)*100
```
### 1.a: the type-token ratio for Reagan's 1985 inaugural address is 31.80%

#b. create a dfm of the two speeches 
##use dfm function to create document feature matrix in which punctuation is removed
```
inauguralDfm <- dfm(corpus_subset(data_corpus_inaugural, President == "Reagan"), remove_punct=TRUE)
```
## calculate cosine similarity between two documents 
?textstat_simil
tstat_inaugural <- textstat_simil(inauguralDfm, inauguralDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
### 1.b:the cosine similarity between the two documents is 95.9% 

#2 
#2.a.1 Stemming the words? 
Since stemming involves reducing words to a common root by chopping off affixes, I hypothesize that stemming 
will decrease the type to token ratio. The TTR will be lower because the affixes of tokens and types will be chopped off to reflect their common root, which will decrease the number of types while keeping the number of tokens constant. Since the numerator will decrease and the denominator will stay the same, the TTR will decrease. Stemming will not affect the cosine similarity of the two documents because cosine similarity is unaffected by the number of common words across documents. 

#bring back reaganinaugurals for reference
```
reaganinaugurals <- corpus_subset(data_corpus_inaugural, President == "Reagan")
```
#stem reaganinaugurals
```
reaganinaugurals_stemmed <- stemDocument(reaganinaugurals, language = "english")
```
#transform reagan inaugurals into corpus
```
reagan_stem <- corpus(reaganinaugurals_stemmed)
```
#find number of text/tokens for each doc using summary()
```
summary(reagan_stem)
```
#TTR 1981 (stemmed)
```
(860/2780)*100
```
###2.a.2: the TTR for Reagan's 1981 inaugural is 30.94%, which is slightly lower than the unstemmed TTR 
#TTR 1985 (stemmed)
```
(878/2908)*100
```
###2.a.2: the TTR for Reagan's 1985 inaugural is 30.20%, which is slightly lower than the unstemmed TTR
#2.a.3
#create dfm and calculate cosine similarity 
```
stemmed_inauguralDfm <- dfm(corpus_subset(data_corpus_inaugural, President == "Reagan"), remove_punct=TRUE, stem=TRUE)
tstat_inaugural <- textstat_simil(stemmed_inauguralDfm, stemmed_inauguralDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
```
### 2.a.3: the cosine similarity is the the same for the stemmed and unstemmed dfm: .96 (stemmed) compared to .959 (unstemmed)

#2.b.1 Stop words 
I hypothesize that removing stopwards will increase the TTR of the two documents.The TTR will go up because the number of types will increase relative to the number of tokens. This is because excluding stopwards will reduce the overall volume of the text by reducing the number of tokens. I hypothesize that removing stopwards will reduce the similarity of the two documents because prepositions and other common words that likely appear across both texts will be removed. 
#create dfm to calculate TTR and cosine similarity 
```
reaganinaugural_stopDfm <- dfm(reaganinaugurals, remove_punct = TRUE, remove = stopwords("english"))
```
#calculate TTR using textstat_lexdiv()
```
textstat_lexdiv(reaganinaugural_stopDfm)
```
###2.b.2: the TTR for Reagan's 1981 inaugural is 64.6% and the TTR for Reagan's 1985 inaugural is 58.7%, which is double the TTR of the the unstemmed version
#calculate cosine similarity 
```
reagan_tstat_inaugural_nostop <- textstat_simil(reaganinaugural_stopDfm, reaganinaugural_stopDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
```
###2.b.3: the cosine similarity is .703, which is .257 lower than the unstemmed and stemmed version

#2.c.1 Lowercase 
I hypothesize that transforming the documents to all lower case will reduce the TTR by reducing the number of types while keeping the number of tokens constant. Since the numerator will decrease and the denominator stays the same, the ratio will decrease. I do not expect that transforming the documents to all lowercase will affect the cosine similarity between the two documents because the words will be the same regardless of whether they were capitalized or not. 
#convert to lowercase and summarize to get type/token info
```
reaganinaugural_lower <- tolower(reaganinaugurals)
summary(reaganinaugural_lower)
```
##TTR 1981 (lower)
```
(850/2780)*100
```
###2.c.2: the TTR for Reagan's 1981 inaugural is 30.56%, which is lower than the unstemmed and no stopwords version 
##TTR 1985 (lower) 
```
(876/2909)*100
```
###2.c.2: the TTR for Reagan's 1985 inaugural is 30.11% which is lower than the unstemmed and no stopwords version 
#transform into dfm and calculate cosine similarity 
```
reaganinaugural_lowerDfm <- dfm(reaganinaugural_lower, remove_punct = TRUE)
reagan_tstat_inaugural_lower <- textstat_simil(reaganinaugural_lowerDfm, reaganinaugural_lowerDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
```
###2.c.3 the cosine similarity between the two documents is .959 after transformation, which is the same as the stemmed and unstemmed versions

#2.4 tf-idf 
Since we have calculated the TTR and calculated word frequency statistics for the two documents with different levels of pre-processing, I think it makes sense to see how tf-idf changes the similarity of the documents. One reason I would use tf-idf is that these documents are naturally similar because they were written by the same person and delivered at the same event. because of this, it is likely that there are many words that are used frequently across both documents, such as prepositions and other words that reflect stylistic choices, as well as words about patriotism and the United States that might be common across the documents because they are both inaugural addresses. However, because these documents were delivered at different times in different political climates, there are certainly words that are used more in some than others that might reveal something about each text. If we wanted to learn more about howReagan's rhetoric or priorities shifted across his two terms, tf-idf would be a good first step because it would allow us to see which words were particularly significant for each document.. 

#3
sentences: 1: "Trump Says He's 'Not Happy' With Border Deal, but Doesn't Say if He Will Sign It." 
2: "Trump 'not happy' with border deal, weighing options for building wall."
## PRE-PROCESSING: convert text to lowercase and remove punctuation so that it is easier to see common terms
```
text <- (c(s1 = "Trump Says He's 'Not Happy' With Border Deal, but Doesn't Say if He Will Sign It.", 
         s2 = "Trump 'not happy' with border deal, weighing options for building wall."))
         ```
#use dfm to represent sentences numerically 
```
dfm_sentences <- dfm(tolower(text), remove_punct = TRUE)
```
#3.a Calculate Euclidean distance. I performed the addition and substractino and squaring on paper
```
sqrt(15)
```
## euclidean distance = 3.87
#3.b Calculate Manhattan distance 
##Manhattan distance is 10
#3.c Calculate cosine similarity 
```
vector1 <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0)
vector2 <- c(1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0)
```
#product of two vectors: 1+1+1+1+1+1+1 = 7
#use formula: (x*y)/(||x||*||y||)
```
cosinesimilarity <- (7)/(16*11)
```
#cosine similarity is .0398 
