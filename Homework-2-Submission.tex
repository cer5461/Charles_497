% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Homework 2 Submission},
  pdfauthor={Charles Ryan},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Homework 2 Submission}
\author{Charles Ryan}
\date{3/9/2021}

\begin{document}
\maketitle

1

\#\#take a glimpse of data\_corpus\_inaugural

\begin{verbatim}
summary(data_corpus_inaugural)
\end{verbatim}

\hypertarget{subset-inaugurals-corpus-to-only-include-speeches-by-reagan}{%
\subsection{subset inaugurals corpus to only include speeches by
Reagan}\label{subset-inaugurals-corpus-to-only-include-speeches-by-reagan}}

\begin{verbatim}
reaganinaugurals <- corpus_subset(data_corpus_inaugural, President == "Reagan")
summary(reaganinaugurals)
\end{verbatim}

\#1.a: calculate the TTR of each of these speeches based on token/type
data from summary(reaganinaugurals) and report findings \#\# TTR 1981

\begin{verbatim}
(902/2780)*100
\end{verbatim}

ANSWER 1.a: the type-token ratio for Reagan's 1981 inaugural address is
32.45\% \#\# TTR 1985

\begin{verbatim}
(925/2909)*100
\end{verbatim}

1.a: the type-token ratio for Reagan's 1985 inaugural address is 31.80\%

\#1.b: create a dfm of the two speeches

\#\#use dfm function to create document feature matrix in which
punctuation is removed

\begin{verbatim}
inauguralDfm <- dfm(corpus_subset(data_corpus_inaugural, President == "Reagan"), remove_punct=TRUE)
\end{verbatim}

\hypertarget{calculate-cosine-similarity-between-two-documents}{%
\subsection{calculate cosine similarity between two
documents}\label{calculate-cosine-similarity-between-two-documents}}

\begin{verbatim}
?textstat_simil
tstat_inaugural <- textstat_simil(inauguralDfm, inauguralDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
\end{verbatim}

ANSWER 1.b:the cosine similarity between the two documents is 95.9\%

2

2.a.1 Stemming the words?

ANSWER 2.A.1: Since stemming involves reducing words to a common root by
chopping off affixes, I hypothesize that stemming will decrease the type
to token ratio. The TTR will be lower because the affixes of tokens and
types will be chopped off to reflect their common root, which will
decrease the number of types while keeping the number of tokens
constant. Since the numerator will decrease and the denominator will
stay the same, the TTR will decrease. Stemming will not affect the
cosine similarity of the two documents because cosine similarity is
unaffected by the number of common words across documents.

\#bring back reaganinaugurals for reference

\begin{verbatim}
reaganinaugurals <- corpus_subset(data_corpus_inaugural, President == "Reagan")
\end{verbatim}

\#stem reaganinaugurals

\begin{verbatim}
reaganinaugurals_stemmed <- stemDocument(reaganinaugurals, language = "english")
\end{verbatim}

\#transform reagan inaugurals into corpus

\begin{verbatim}
reagan_stem <- corpus(reaganinaugurals_stemmed)
\end{verbatim}

\#find number of text/tokens for each doc using summary()

\begin{verbatim}
summary(reagan_stem)
\end{verbatim}

\#TTR 1981 (stemmed)

\begin{verbatim}
(860/2780)*100
\end{verbatim}

ANSWER 2.a.2: the TTR for Reagan's 1981 inaugural is 30.94\%, which is
slightly lower than the unstemmed TTR

\#TTR 1985 (stemmed)

\begin{verbatim}
(878/2908)*100
\end{verbatim}

ANSWER 2.a.2: the TTR for Reagan's 1985 inaugural is 30.20\%, which is
slightly lower than the unstemmed TTR

\#2.a.3

\#create dfm and calculate cosine similarity

\begin{verbatim}
stemmed_inauguralDfm <- dfm(corpus_subset(data_corpus_inaugural, President == "Reagan"), remove_punct=TRUE, stem=TRUE)
tstat_inaugural <- textstat_simil(stemmed_inauguralDfm, stemmed_inauguralDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
\end{verbatim}

ANSWER 2.a.3: the cosine similarity is the the same for the stemmed and
unstemmed dfm: .96 (stemmed) compared to .959 (unstemmed)

ANSWER 2.b.1 Stop words: I hypothesize that removing stopwards will
increase the TTR of the two documents.The TTR will go up because the
number of types will increase relative to the number of tokens. This is
because excluding stopwards will reduce the overall volume of the text
by reducing the number of tokens. I hypothesize that removing stopwards
will reduce the similarity of the two documents because prepositions and
other common words that likely appear across both texts will be removed.

\#create dfm to calculate TTR and cosine similarity

\begin{verbatim}
reaganinaugural_stopDfm <- dfm(reaganinaugurals, remove_punct = TRUE, remove = stopwords("english"))
\end{verbatim}

\#calculate TTR using textstat\_lexdiv()

\begin{verbatim}
textstat_lexdiv(reaganinaugural_stopDfm)
\end{verbatim}

ANSWER 2.b.2: the TTR for Reagan's 1981 inaugural is 64.6\% and the TTR
for Reagan's 1985 inaugural is 58.7\%, which is double the TTR of the
the unstemmed version

\#calculate cosine similarity

\begin{verbatim}
reagan_tstat_inaugural_nostop <- textstat_simil(reaganinaugural_stopDfm, reaganinaugural_stopDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
\end{verbatim}

ANSWER 2.b.3: the cosine similarity is .703, which is .257 lower than
the unstemmed and stemmed version

ANSWER 2.c.1 Lowercase: I hypothesize that transforming the documents to
all lower case will reduce the TTR by reducing the number of types while
keeping the number of tokens constant. Since the numerator will decrease
and the denominator stays the same, the ratio will decrease. I do not
expect that transforming the documents to all lowercase will affect the
cosine similarity between the two documents because the words will be
the same regardless of whether they were capitalized or not.

\#convert to lowercase and summarize to get type/token info

\begin{verbatim}
reaganinaugural_lower <- tolower(reaganinaugurals)
summary(reaganinaugural_lower)
\end{verbatim}

\#\#TTR 1981 (lower)

\begin{verbatim}
(850/2780)*100
\end{verbatim}

ANSWER 2.c.2: the TTR for Reagan's 1981 inaugural is 30.56\%, which is
lower than the unstemmed and no stopwords version

\#\#TTR 1985 (lower)

\begin{verbatim}
(876/2909)*100
\end{verbatim}

ANSWER 2.c.2: the TTR for Reagan's 1985 inaugural is 30.11\% which is
lower than the unstemmed and no stopwords version

\#transform into dfm and calculate cosine similarity

\begin{verbatim}
reaganinaugural_lowerDfm <- dfm(reaganinaugural_lower, remove_punct = TRUE)
reagan_tstat_inaugural_lower <- textstat_simil(reaganinaugural_lowerDfm, reaganinaugural_lowerDfm[c("1981-Reagan", "1985-Reagan"),], margin = "documents", method = "cosine")
\end{verbatim}

ANSWER 2.c.3: the cosine similarity between the two documents is .959
after transformation, which is the same as the stemmed and unstemmed
versions

ANSWER 2.4 tf-idf: Since we have calculated the TTR and calculated word
frequency statistics for the two documents with different levels of
pre-processing, I think it makes sense to see how tf-idf changes the
similarity of the documents. One reason I would use tf-idf is that these
documents are naturally similar because they were written by the same
person and delivered at the same event. because of this, it is likely
that there are many words that are used frequently across both
documents, such as prepositions and other words that reflect stylistic
choices, as well as words about patriotism and the United States that
might be common across the documents because they are both inaugural
addresses. However, because these documents were delivered at different
times in different political climates, there are certainly words that
are used more in some than others that might reveal something about each
text. If we wanted to learn more about howReagan's rhetoric or
priorities shifted across his two terms, tf-idf would be a good first
step because it would allow us to see which words were particularly
significant for each document..

\#3

1: ``Trump Says He's `Not Happy' With Border Deal, but Doesn't Say if He
Will Sign It.'' 2: ``Trump `not happy' with border deal, weighing
options for building wall.''

\hypertarget{pre-processing-convert-text-to-lowercase-and-remove-punctuation-so-that-it-is-easier-to-see-common-terms}{%
\subsection{PRE-PROCESSING: convert text to lowercase and remove
punctuation so that it is easier to see common
terms}\label{pre-processing-convert-text-to-lowercase-and-remove-punctuation-so-that-it-is-easier-to-see-common-terms}}

\begin{verbatim}
text <- (c(s1 = "Trump Says He's 'Not Happy' With Border Deal, but Doesn't Say if He Will Sign It.", 
         s2 = "Trump 'not happy' with border deal, weighing options for building wall."))
         ```
#use dfm to represent sentences numerically 
\end{verbatim}

dfm\_sentences \textless- dfm(tolower(text), remove\_punct = TRUE)

\begin{verbatim}
#3.a Calculate Euclidean distance. I performed the addition and substraction and squaring on paper
\end{verbatim}

sqrt(15)

\begin{verbatim}
ANSWER 3.a: euclidean distance = 3.87

#3.b Calculate Manhattan distance (I did this calculation on paper) wiht formula |x1 - x2| + |y1-y2|... for n

ANSWER 3.b: Manhattan distance is 15 because there are 15 instances of 0,1 between Xs and Ys

#3.c Calculate cosine similarity 

#vectorize sentences based on frequencies from dfm 
\end{verbatim}

vector1 \textless- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0) vector2
\textless- c(1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0)

\begin{verbatim}
#product of two vectors: 1+1+1+1+1+1+1 = 7
#use formula: (x*y)/(||x||*||y||)
\end{verbatim}

cosinesimilarity \textless- (7)/(16*11) ``` ANSWER 3.c: cosine
similarity is .0398

\end{document}
